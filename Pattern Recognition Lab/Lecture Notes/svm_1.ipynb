{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section intoriduces another linear classifier. Consider the scenario below. <img src = 'svm_1.png', width=400, height = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the three separating hyperplanes the red one in the middle is obvioulsy better as a classifier, since it is away from both class data and hence chance of a test point to fall on the wrong side is lower. This intuition is what governs the formulation of SVMs. \n",
    "\n",
    "The intuition can be translated to the heuristic : *** Find a separting hyperplane  whose distance to its nearest training vector is as large as possible. ***\n",
    "\n",
    "Now we need to express this heuristic mathematically so that the solution of the model leads to such a hyperplane. For this we need to find the distance of a point $x_0$ to a hyperplane $w^Tx + b = 0$.\n",
    "\n",
    "Therefore we will first derive an expression for the distance of a point to a hyperplane. As shown in the figure below the parameter vector $w$ is normal to the hyperplane.\n",
    "<img src = 'svm_2.png', width=400, height = 400> (the dark arrow from origin).\n",
    "If we denote by $\\hat x$ the nearest point on the hyperplane to the given point $x_0$. Then the vector $x_0 - \\hat x $ should be normal to the plane and hence in the direction of $w$.\n",
    "Therefore we can write $x_0 - \\hat x = \\alpha w$ where $\\alpha \\in \\Re$ is a scalar. The distance of the point $x_0$ from the plane is given by $\\lVert x_0 - \\hat x \\rVert = \\lvert \\alpha \\rvert  \\lVert w \\rVert$. \n",
    "\n",
    "\n",
    "\n",
    "Note that since $\\hat x$ is a point on the hyperplane $w^T \\hat x + b = 0$. Therefore, $w^T (x_0 - \\hat x) = (w^Tx_0 + b)$. But since $x_0 - \\hat x = \\alpha w$ we have $w^T (x_0 - \\hat x) = \\alpha  \\lVert w \\rVert^2 $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that the distance is given by $\\lvert \\alpha \\rvert  \\lVert w \\rVert = \\frac{\\lvert w^Tx_0 + b \\rvert}{\\lVert w \\rVert}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now coming back to the support vector machine intuition, we will formulate it using the distance equation above. Assume we have training data $X = \\{x_1,x_2, \\cdots x_n \\} $ . The minimum distance of a hyperplane parameterized by $w,b$ to the training data is :\n",
    "        \\begin{equation}\n",
    "          \\min_i \\frac{\\lvert w^Tx_i + b \\rvert}{\\lVert w \\rVert}\n",
    "        \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that he heuristic is to maximize the minimum distance to the training data. Let $Y=\\{y_1,y_2, \\cdots y_n\\}$ be the class labels. Here we use a different labelling from the one we have been used to till now. Let $y_i \\in \\{-1,1\\}$. We call points that fall on the negative side of the hyperplane $-1$ and the ones that fall on the positive side $+1$.\n",
    "So we need our hyperplane to satisfy $y_i(w^Tx_i + b) > 0; \\; \\forall i$  (It needs to separate the two classes)\n",
    "\n",
    "We combine all these considerations and formulate the optimization problem below:\n",
    "\\begin{equation}\n",
    "         \\max_{w,b} \\min_i \\frac{\\lvert w^Tx_i + b \\rvert}{\\lVert w \\rVert}\n",
    "        \\end{equation}\n",
    "        subject to \n",
    "        \\begin{equation}\n",
    "          y_i(w^Tx_i + b) > 0 \\; \\forall i = 1 ...n \n",
    "        \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will simplify this to get more easily solvable problem. For that we make use of the following observation.\n",
    "\n",
    "If $w,b$ is an optimal solution for the problem above then scaling $w$ and $b$ by any positive real number $c$ does not change its optimality, that is, $w/c,b/c$ is also optimal. (The reader is invited to check that the constraints hold because the division is dine by a positive number and the optimal objective function value does not change on the scaling)\n",
    "\n",
    "So if we choose $c$ to be $min\\{ \\lvert w^Tx_i + b  \\rvert \\; i=1..n  \\}$, then with respect to the scaled parameters \n",
    "$w' = w/c, b'=b/c $ the constraints will be changed to :\n",
    "       \\begin{equation}\n",
    "          y_i(w'^Tx_i + b') \\geq 1 \\; \\forall i = 1 ...n \n",
    "        \\end{equation}\n",
    " And the objective function value  will be $\\frac{1}{\\lVert w' \\rVert } $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we instead of the original formulation we will go for a modified one given by :<br>\n",
    "\n",
    "\\begin{equation}\n",
    "         \\max_{w,b} \\frac{1}{\\lVert w \\rVert }\n",
    "        \\end{equation}\n",
    "        \n",
    "subject to \n",
    "        \\begin{equation}\n",
    "          y_i(w^Tx_i + b) \\geq 1 \\; \\forall i = 1 ...n \n",
    "        \\end{equation}\n",
    "         \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reader is requested to go through the lecture notes to convince that both have the same solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are more used to minimization problems we can minimize $\\lVert w \\rVert $ rather than maximize $\\frac{1}{\\lVert w \\rVert} $. So the final version that we are interested is :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "         \\min_{w,b} \\lVert w \\rVert \n",
    "        \\end{equation}\n",
    "        \n",
    "subject to \n",
    "        \\begin{equation}\n",
    "          y_i(w^Tx_i + b) \\geq 1 \\; \\forall i = 1 ...n \n",
    "        \\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
