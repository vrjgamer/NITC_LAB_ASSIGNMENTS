{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will use the dataset that we had used for the second assignment for this one also. \n",
    "- The problem is to find a linear separating hyper-plane using logistic regression.\n",
    "- Use tensorflow to implement the gradient descent procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  100 loss: 107.252663 train_acc: 0.353160 test_acc: 0.352174\n",
      "epoch:  200 loss: 25.133774 train_acc: 0.669145 test_acc: 0.682609\n",
      "epoch:  300 loss: 150.254105 train_acc: 0.351301 test_acc: 0.347826\n",
      "epoch:  400 loss: 108.644943 train_acc: 0.646840 test_acc: 0.665217\n",
      "epoch:  500 loss: 27.511677 train_acc: 0.591078 test_acc: 0.526087\n",
      "epoch:  600 loss: 145.663071 train_acc: 0.646840 test_acc: 0.660870\n",
      "epoch:  700 loss: 114.564415 train_acc: 0.646840 test_acc: 0.660870\n",
      "epoch:  800 loss: 111.378418 train_acc: 0.646840 test_acc: 0.660870\n",
      "epoch:  900 loss: 120.644424 train_acc: 0.644981 test_acc: 0.660870\n",
      "epoch: 1000 loss: 27.503599 train_acc: 0.555762 test_acc: 0.465217\n",
      "epoch: 1100 loss: 26.613510 train_acc: 0.657993 test_acc: 0.669565\n",
      "epoch: 1200 loss: 95.761894 train_acc: 0.360595 test_acc: 0.352174\n",
      "epoch: 1300 loss: 24.098326 train_acc: 0.557621 test_acc: 0.478261\n",
      "epoch: 1400 loss: 133.615540 train_acc: 0.646840 test_acc: 0.660870\n",
      "epoch: 1500 loss: 118.746330 train_acc: 0.646840 test_acc: 0.665217\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "\n",
    "#reads the csv file and returns a numpy array dataset of dimensions Nx(d+1). N is the number of random vectors and \n",
    "#d is the dimension. dataset has N rows each one being an observation of dimension d. the (d+1)th column \n",
    "# contains the labels.\n",
    "def loadCsv(filename):\n",
    "    lines = csv.reader(open(filename, \"rt\"))#read the file\n",
    "    dataset = list(lines)\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i] = [float(x) for x in dataset[i]] #converting to floating point numbers from integers\n",
    "    dataset = np.asfarray(dataset)#list to numpy array conversion\n",
    "    return dataset\n",
    "\n",
    "# splits the entire set into training and test set. training set will contain splitratio x N number of observations\n",
    "# randomly chosen from the set. the rest will go to the test set.\n",
    "def splitDataset(dataset, splitratio):\n",
    "    trainsize = int(np.round(dataset.shape[0]*splitratio))\n",
    "    trainset = np.zeros((trainsize,dataset.shape[1]))#array to store the training set.\n",
    "    testset = deepcopy(dataset)#create a copy of the dataset in test set.\n",
    "    for numsamples in range(trainsize):\n",
    "        indx = np.random.randint(0,testset.shape[0])#random index generation\n",
    "        trainset[numsamples,:] = testset[indx,:]#adding the randomly selected data vector to the training set\n",
    "        testset = np.delete(testset, indx, axis = 0)#delete the selected observation from the test set.\n",
    "    return trainset,testset\n",
    "\n",
    "dataset = loadCsv(\"pima-indians-diabetes.csv\")\n",
    "trainset, testset = splitDataset(dataset,0.7)\n",
    "\n",
    "learning_rate = 0.03\n",
    "training_epochs = 1500\n",
    "batch_size = 200\n",
    "display_step = 100\n",
    "\n",
    "X_train = trainset[:,0:8]\n",
    "#Y_train = np.concatenate((trainset[:,-1].reshape(1,X_train.shape[0]),1-trainset[:,-1].reshape(1,X_train.shape[0]))).T\n",
    "Y_train = trainset[:,-1]\n",
    "X_test = testset[:,0:8]\n",
    "#Y_test = np.concatenate((testset[:,-1].reshape(1,X_test.shape[0]),1-testset[:,-1].reshape(1,X_test.shape[0]))).T\n",
    "Y_test = testset[:,-1]\n",
    "\n",
    "data = tf.placeholder(tf.float32, [None, 8])\n",
    "target = tf.placeholder(tf.float32, [None, 1])\n",
    "#W = tf.Variable(tf.random_normal([8, 1], mean=0.0, stddev=0.05))\n",
    "W = tf.Variable(tf.random_normal(shape=[8, 1]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1]))\n",
    "z = tf.matmul(data, W) + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=z,labels=target))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "prediction = tf.round(tf.sigmoid(z))\n",
    "correct = tf.cast(tf.equal(prediction, target), dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(correct)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        batch_index = np.random.choice(len(X_train), size=batch_size)\n",
    "        batch_train_X = X_train[batch_index]\n",
    "        batch_train_y = np.matrix(Y_train[batch_index]).T\n",
    "        sess.run(optimizer, feed_dict={data: batch_train_X, target: batch_train_y})\n",
    "        temp_loss = sess.run(cost, feed_dict={data: batch_train_X, target: batch_train_y})\n",
    "        temp_train_acc = sess.run(accuracy, feed_dict={data: X_train, target: np.matrix(Y_train).T})\n",
    "        temp_test_acc = sess.run(accuracy, feed_dict={data: X_test, target: np.matrix(Y_test).T})\n",
    "        if (epoch + 1) % display_step == 0:\n",
    "            print('epoch: {:4d} loss: {:5f} train_acc: {:5f} test_acc: {:5f}'.format(epoch + 1, temp_loss,temp_train_acc, temp_test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there are more than two classes then we have to modify the basic logistic regression procedure to account for the different classes. In this case $Y \\in \\{0,1,2,\\cdots K-1 \\} $ when there are $K$ classes. Here we model the conditional probability of class label to be $k$ as : $P(y=k | x) = \\frac{e^{w_k^Tx}}{1+e^{w_k^Tx}}$. This makes the system more complex than logistic regression as there are $k$ sets of parameters $\\{w_1,w_2,  \\cdots w_K \\}$. We will have to frame \n",
    "the expression for negative log likelihood interms of these parameters:\n",
    "\\begin{equation}\n",
    "  nll = -\\sum_i^n \\delta(y_i,k) log(P(y_i=k | x_i) = -\\sum_i^n \\delta(y_i,k) \\frac{e^{w_k^Tx_i}}{1+e^{w_k^Tx_i}}\n",
    "\\end{equation}\n",
    "where $\\delta(y_i,k) = 1$ if $y_i=k$ and $0$ otherwise. This is called **Softmax Regression**\n",
    "\n",
    "We perform gradient descent on nll with respect to both parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mnist'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c3830399adb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmnist\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m '''\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mnist'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mnist import MNIST\n",
    "\n",
    "'''\n",
    "\n",
    "download mnist data from \"http://yann.lecun.com/exdb/mnist/\" to a directory. Extract them and rename the files as :\n",
    "train-images.idx3-ubyte:  training set images   -----> train-images-idx3-ubyte\n",
    "train-labels.idx1-ubyte:  training set labels   -------> train-labels-idx1-ubyte\n",
    "t10k-images.idx3-ubyte:   test set images    --------> t10k-images-idx3-ubyte\n",
    "t10k-labels.idx1-ubyte: test image labels ----------> t10k-labels-idx1-ubyte\n",
    "\n",
    "Each data sample is a vector of dimensions 784. The vectors are handwritten digit images of size 28x28 vectorized to \n",
    "784 dimensional vector. The labels are 0 to 9. \n",
    "\n",
    "First we will do a logistic regression with two classes or two digits. Select 5 and 1. They have very dissimilar shapes\n",
    "hopefully more easily classifiable than, say 5 and 6. For that use the choose_digits functions which returns two sets of \n",
    "vectors corresponding to the two chosen digits.\n",
    "\n",
    "Using this set data we will do logistic regression and get it tested using the testing data also provided.\n",
    "\n",
    "'''\n",
    "mndata = MNIST('data_directory')\n",
    "img_tr,img_label = mndata.load_training()\n",
    "img_test, img_test_label = mndata.load_testing()\n",
    "\n",
    "img_tr = np.array(img_tr)\n",
    "img_label = np.array(img_label)\n",
    "\n",
    "img_test = np.array(img_test)\n",
    "img_test_label = np.array(img_test_label)\n",
    "\n",
    "def choose_digits(digit_1, digit_2, x_data, y_data):\n",
    "    x_tr_1 = x_data[y_data==digit_1,:]\n",
    "    x_tr_2 = x_data[y_data==digit_2,:]\n",
    "    return x_tr_1, x_tr_2\n",
    "\n",
    "def merge(X1,X2):\n",
    "    X_merged = np.zeros((X1.shape[0]+X2.shape[0],X1.shape[1]))\n",
    "    X_labels = np.zeros((X1.shape[0]+X2.shape[0],1))\n",
    "    counter = 0;\n",
    "    i = 0\n",
    "    while(counter < len(X1) and counter < len(X2)):\n",
    "        X_merged[i] = X1[counter]\n",
    "        X_merged[i+1] = X2[counter]\n",
    "        X_labels[i+1] = 1\n",
    "        i += 2\n",
    "        counter += 1\n",
    "    if(counter == len(X1)):\n",
    "        while(counter < len(X2)):\n",
    "            X_merged[i] = X2[counter]\n",
    "            X_labels[i] = 1\n",
    "            i += 1\n",
    "            counter += 1\n",
    "    else:\n",
    "        while(counter < len(X1)):\n",
    "            X_merged[i] = X1[counter]\n",
    "            i += 1\n",
    "            counter += 1\n",
    "    return X_merged,X_labels\n",
    "\n",
    "x_tr_1,x_tr_2 = choose_digits(5,1, img_tr,img_label)\n",
    "\n",
    "X_train,Y_train = merge(x_tr_1,x_tr_2)\n",
    "\n",
    "x_test_1,x_test_2 = choose_digits(5,1,img_test,img_test_label)\n",
    "\n",
    "X_test,Y_test = merge(x_test_1,x_test_2)\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "data = tf.placeholder(tf.float32, [None, 784])\n",
    "target = tf.placeholder(tf.float32, [None, 1])\n",
    "#W = tf.Variable(tf.random_normal([8, 2], mean=0.0, stddev=0.05))\n",
    "W = tf.Variable(tf.random_normal(shape=[784, 1]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1]))\n",
    "z = tf.matmul(data, W) + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=z,labels=target))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "prediction = tf.round(tf.sigmoid(z))\n",
    "correct = tf.cast(tf.equal(prediction, target), dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(correct)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        batch_index = np.random.choice(len(X_train), size=batch_size)\n",
    "        batch_train_X = X_train[batch_index]\n",
    "        batch_train_y = np.matrix(Y_train[batch_index])\n",
    "        sess.run(optimizer, feed_dict={data: batch_train_X, target: batch_train_y})\n",
    "        temp_loss = sess.run(cost, feed_dict={data: batch_train_X, target: batch_train_y})\n",
    "        temp_train_acc = sess.run(accuracy, feed_dict={data: X_train, target: np.matrix(Y_train)})\n",
    "        temp_test_acc = sess.run(accuracy, feed_dict={data: X_test, target: np.matrix(Y_test)})\n",
    "        if (epoch + 1) % display_step == 0:\n",
    "            print('epoch: {:4d} loss: {:5f} train_acc: {:5f} test_acc: {:5f}'.format(epoch + 1, temp_loss,temp_train_acc, temp_test_acc))\n",
    "    parameter = sess.run(W)\n",
    "    \n",
    "plt.imshow(parameter.reshape((28,28)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assign class label y=0 to digit_1 and y=1 to digit_2\n",
    "- Perform logistic regression classification on the data\n",
    "- Compute the accuracy of classification\n",
    "- The weight vector $w$ is 784 long vector, reshape it to 28xc28 and display it.\n",
    "- Modify the program to make it softmax regression.(Now there will be two parameters $w_1$ and $w_2$)\n",
    "- Display both the parameters and display them as images of size 28x28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modify choose_digits function to retrieve more than two digits and perform softmax regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
